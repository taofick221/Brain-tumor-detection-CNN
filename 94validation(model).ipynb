{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gkK0lReMOX53T2Qe2At3hkjQ71r7l_Ku",
      "authorship_tag": "ABX9TyP6/SanlTZ0NVtDsksHuDG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taofick221/Brain-tumor-detection-CNN/blob/main/94validation(model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess code\n"
      ],
      "metadata": {
        "id": "zZSeCeGSZU4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Paths\n",
        "input_base_path = '/content/drive/MyDrive/Thesis/dataset2.0'\n",
        "output_base_path = '/content/drive/MyDrive/Thesis/preprocess_data3.0'\n",
        "\n",
        "\n",
        "# Image size and augmentation settings\n",
        "IMG_SIZE = (224, 224)\n",
        "AUG_PER_IMAGE = 5\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create output folders\n",
        "for split in ['Training', 'Testing']:\n",
        "    split_input = os.path.join(input_base_path, split)\n",
        "    split_output = os.path.join(output_base_path, split)\n",
        "\n",
        "    for class_name in os.listdir(split_input):\n",
        "        input_folder = os.path.join(split_input, class_name)\n",
        "        output_folder = os.path.join(split_output, class_name)\n",
        "        os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "        print(f\"ğŸ“‚ Processing Class: {split}/{class_name}\")\n",
        "\n",
        "        for img_name in os.listdir(input_folder):\n",
        "            img_path = os.path.join(input_folder, img_name)\n",
        "\n",
        "            try:\n",
        "                img = load_img(img_path, target_size=IMG_SIZE)\n",
        "                x = img_to_array(img)\n",
        "                x = x.reshape((1,) + x.shape)\n",
        "\n",
        "                # Save original image as well (resized)\n",
        "                img.save(os.path.join(output_folder, img_name))\n",
        "\n",
        "                # Generate augmented images\n",
        "                i = 0\n",
        "                for batch in datagen.flow(\n",
        "                        x,\n",
        "                        batch_size=1,\n",
        "                        save_to_dir=output_folder,\n",
        "                        save_prefix=f'{class_name}_aug',\n",
        "                        save_format='jpg'):\n",
        "                    i += 1\n",
        "                    if i >= AUG_PER_IMAGE:\n",
        "                        break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Skipping {img_name} due to error: {e}\")\n",
        "\n",
        "print(\"âœ… All preprocessing & augmentation completed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQrm5WXaZZvG",
        "outputId": "14807dda-4ac5-42cb-b4c2-50cf43356911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Processing Class: Training/notumor\n",
            "ğŸ“‚ Processing Class: Training/meningioma\n",
            "ğŸ“‚ Processing Class: Training/pituitary\n",
            "ğŸ“‚ Processing Class: Training/glioma\n",
            "ğŸ“‚ Processing Class: Testing/pituitary\n",
            "ğŸ“‚ Processing Class: Testing/notumor\n",
            "ğŸ“‚ Processing Class: Testing/meningioma\n",
            "ğŸ“‚ Processing Class: Testing/glioma\n",
            "âœ… All preprocessing & augmentation completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training\n"
      ],
      "metadata": {
        "id": "BU9gLW6yDxGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import os\n",
        "\n",
        "# === CONFIG ===\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# === PATHS ===\n",
        "train_path = '/content/drive/MyDrive/Thesis/preprocess_data3.0/Training'\n",
        "val_path = '/content/drive/MyDrive/Thesis/preprocess_data3.0/Testing'\n",
        "model_save_path = '/content/drive/MyDrive/Thesis/final_model/best_custom_model.keras'\n",
        "\n",
        "# === DATA LOADING ===\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_path,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# === MODEL ===\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),  # Prevent overfitting\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# === CALLBACKS ===\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_save_path,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "lr_schedule = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# === TRAIN MODEL ===\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint, early_stop, lr_schedule]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQYe_7rAD24A",
        "outputId": "30654699-88eb-481c-e828-41a071665082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 26151 images belonging to 4 classes.\n",
            "Found 7340 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.6410 - loss: 0.8386\n",
            "Epoch 1: val_accuracy improved from -inf to 0.75095, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9636s\u001b[0m 12s/step - accuracy: 0.6411 - loss: 0.8384 - val_accuracy: 0.7510 - val_loss: 0.6093 - learning_rate: 5.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.8260 - loss: 0.4422\n",
            "Epoch 2: val_accuracy improved from 0.75095 to 0.82262, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3510s\u001b[0m 4s/step - accuracy: 0.8261 - loss: 0.4421 - val_accuracy: 0.8226 - val_loss: 0.4516 - learning_rate: 5.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8821 - loss: 0.2985\n",
            "Epoch 3: val_accuracy improved from 0.82262 to 0.87834, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3014s\u001b[0m 4s/step - accuracy: 0.8821 - loss: 0.2985 - val_accuracy: 0.8783 - val_loss: 0.2962 - learning_rate: 5.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9232 - loss: 0.1981\n",
            "Epoch 4: val_accuracy improved from 0.87834 to 0.89905, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2943s\u001b[0m 4s/step - accuracy: 0.9232 - loss: 0.1981 - val_accuracy: 0.8990 - val_loss: 0.2756 - learning_rate: 5.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9464 - loss: 0.1456\n",
            "Epoch 5: val_accuracy improved from 0.89905 to 0.91485, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2932s\u001b[0m 4s/step - accuracy: 0.9464 - loss: 0.1456 - val_accuracy: 0.9149 - val_loss: 0.2380 - learning_rate: 5.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9603 - loss: 0.1035\n",
            "Epoch 6: val_accuracy did not improve from 0.91485\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2944s\u001b[0m 4s/step - accuracy: 0.9603 - loss: 0.1035 - val_accuracy: 0.9041 - val_loss: 0.3001 - learning_rate: 5.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9701 - loss: 0.0780\n",
            "Epoch 7: val_accuracy improved from 0.91485 to 0.92398, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2926s\u001b[0m 4s/step - accuracy: 0.9701 - loss: 0.0780 - val_accuracy: 0.9240 - val_loss: 0.2668 - learning_rate: 5.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9813 - loss: 0.0533\n",
            "Epoch 8: val_accuracy did not improve from 0.92398\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2965s\u001b[0m 4s/step - accuracy: 0.9813 - loss: 0.0533 - val_accuracy: 0.9240 - val_loss: 0.3361 - learning_rate: 2.5000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9851 - loss: 0.0392\n",
            "Epoch 9: val_accuracy improved from 0.92398 to 0.93815, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2993s\u001b[0m 4s/step - accuracy: 0.9852 - loss: 0.0392 - val_accuracy: 0.9381 - val_loss: 0.2605 - learning_rate: 2.5000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9898 - loss: 0.0272\n",
            "Epoch 10: val_accuracy improved from 0.93815 to 0.94019, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2954s\u001b[0m 4s/step - accuracy: 0.9898 - loss: 0.0272 - val_accuracy: 0.9402 - val_loss: 0.2562 - learning_rate: 1.2500e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9934 - loss: 0.0226\n",
            "Epoch 11: val_accuracy did not improve from 0.94019\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2935s\u001b[0m 4s/step - accuracy: 0.9934 - loss: 0.0226 - val_accuracy: 0.9377 - val_loss: 0.2925 - learning_rate: 1.2500e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9926 - loss: 0.0211\n",
            "Epoch 12: val_accuracy improved from 0.94019 to 0.94169, saving model to /content/drive/MyDrive/Thesis/final_model/best_custom_model.keras\n",
            "\u001b[1m818/818\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2948s\u001b[0m 4s/step - accuracy: 0.9926 - loss: 0.0211 - val_accuracy: 0.9417 - val_loss: 0.3071 - learning_rate: 6.2500e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m149/818\u001b[0m \u001b[32mâ”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m37:11\u001b[0m 3s/step - accuracy: 0.9892 - loss: 0.0246"
          ]
        }
      ]
    }
  ]
}